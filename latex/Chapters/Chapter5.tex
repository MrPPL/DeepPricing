% Chapter Template

\chapter{Option Pricing and Deep Learning} % Main chapter title

\label{Chapter5} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

Deep learning can be applied to option pricing theory in different ways. We will investigate two methods using MLP regression. The first method considered is a modified LSM algorithm, where MLP regression is used to estimate the expected continuation value instead of the linear model. The second method uses MLP regression on a data set ($\matr{X}, \bm{y}$) to infer prices. The input parameters $\matr{X}$ are simulated and the target variable $\bm{y}$ is found with existing pricing methods.\\

The two methods will be compared numerically in the next chapter. Both methods fall within supervised regression where we will use MLP introduced in section \ref{multilayerPerceptron} to approximate the mappings. The risky assets are modeled with Black-Scholes theory from earlier chapters, hence, the appropriate simulating methods are already presented (Chapter \ref{Chapter2}, \ref{Chapter3}). The theory in chapter \ref{Chapter4} will be specialized for the specific task. The advantage of MLP is that the model scales well to high dimensional data, where, e.g. polynomial regression (section \ref{LSM}) is prone to overfitting and slow compared to MLP for high dimensional tasks. 

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Multilayer Perceptron Regression for Optimal Stopping}
The first application of neural networks is to investigate, if the LSM method can be improved by using MLP regression to approximate expected continuation value instead of using the linear model in LSM. This section is influenced by \parencite{LSM, Lelong19, KohlerMichael2010}, where the approximate scheme with only in the money paths (ITM) is inspired by \parencite{LSM} and the idea of using neural network instead of the linear model comes from \parencite{KohlerMichael2010, Lelong19}. The algorithm is based on approximating the American put option with a Bermudan option. \\

The setup is the same as for the LSM presented in chapter \ref{Chapter3}, where the difference is the regression to estimate the expected continuation value. The main difference of the two methods lies in the capacity of the two regression models. The MLP through its chain of functions and activation functions can capture complex structures. Another advantage is that deep learning is known for breaking the curse of dimensionality, hence, the MLP could be more suited for multivariate contingent claims than the classical LSM.

\subsection{Recap MLP}
We model the MLP with a nonlinear function:
$$s \in S \in \mathbb{R}^R \mapsto \Psi(s;\theta) \in \mathbb{R}$$
where $\Psi$ is the function decomposition:
\begin{align*}
\Psi = a_{L+1} \circ g_L \circ a_{L} \circ \cdots \circ g_1 \circ a_1 \quad where \ L\in \mathbb{N}
\end{align*}
We refer to chapter \ref{Chapter4} for notation, where $a_l(\bm{x})=\matr{W}_{(l)}^T \bm{x}^{(l-1)} + \bm{w}_{0}^{(l)}$. We collect all the parameters of the different layers into a high dimensional parameter:
$$\theta=(\matr{W}_l, \bm{w}_{0,l})_{l=1,\ldots,L+1} \in \mathbb{R}^{N_m} \text{ with } N_m=\sum_{l=1}^{L+1} m^{(l)} (1+ m^{(l-1)})$$
We will restrict our parameter space by an increasing sequence $(\gamma_p)_{p\in \mathbb{N}}$ such that $\lim_{p\to \infty} \gamma_p=\infty$ and $p\in \mathbb{N}$ is the maximum number of neurons in each hidden layer. We define the set:
$$\Theta_p = \{ \theta \in \mathbb{R} \times \mathbb{R}^p \times (\mathbb{R}^p \times \mathbb{R}^{p \times p})^{L-1} \times \mathbb{R}^{p} \times \mathbb{R}^{p \times p} : |\theta| \leq \gamma_p \}$$
By the definition of the set, we restrict our MLP to the set:
$$\mathcal{N} \mathcal{N}_p= \{ \Psi(\cdot;\theta) : \theta \in \Theta_p \}$$
Unfortunately the $\mathcal{N} \mathcal{N}_p$ is not a vector space, hence, the regression cannot approximate the expected continuation value with a projection onto a finite vector space as for the LSM. The MLP method is justified by the "Universal Approximate Theorem" (theorem \ref{UniversalApproxTheorem})

\begin{theorem}\label{UniversalApproxTheorem}
\textbf{Universal Approximation Theorem} Assume that the activation function $g$ is non constant and bounded. Let $\mu$ denote the probability measure on $\mathbb{R}^r$, then for any $L\geq 1$, then $\mathcal{N} \mathcal{N}_\infty$ is dense in $L^2(\mathbb{R}^r, \mu)$\\
where $\mathcal{N} \mathcal{N}_\infty= \cup_{p\in \mathbb{N}} \mathcal{N} \mathcal{N}_\infty$\\
\null \hfill (p. 4 \parencite{Lelong19})
\end{theorem}
Note that the above theorem can be rephrased in terms of approximating random variables.
\begin{remark}
Let Y be a real valued random variable such that $E[Y^2]< \infty$. Let X be a random variable taking values in $R^r$ and $\mathcal{G}$ be the smallest $\sigma$-algebra such that X is $\mathcal{G}$ measurable. Then, there exists a sequence $(\theta_p)_{p\geq 2} \in \prod_{p=2}^{\infty} \Theta_{p}$ such that $E[|Y-\Psi_p(X;\theta_p)|^2]=0$. Therefore, if for every $p \geq 2$, $\alpha_p \in \Theta_p$ solves
$$\inf_{\theta\in \Theta_p} E[|\Psi_p(X;\theta)-Y|^2]$$
Then the sequence ($(\Psi_p(X;\alpha_p))_{p\geq 2}$) converges to $E[Y|X]$ in $L^2(\Omega)$ when $p \to \infty$ \\
\null \hfill (p. 5 \parencite{Lelong19}).
\end{remark}
The remark reveals that the MLP can be used to approximate the conditional expectation instead of the linear model in LSM.

\subsection{The Algorithm}
The algorithm is similar to the LSM, but the regression step is slightly different. We will use the same assumptions as in LSM section and same principles. Recall that the optimal stopping problem can be solved with dynamic programming principle on the optimal policy:
\begin{equation*}\label{LSMDynamic3}
\begin{split}
\begin{cases}
          \hat{\tau}_{t_N}^{k,p,K} = t_N\\
          \hat{\tau}_{t_n}^{k,p,K} = t_n \cdot 1_{\{G(S^{(k)}(t_n)) \geq \Psi_p(S^{(k)}(t_n) ; \hat{\theta}_{t_n}^{p,K} ) \}} + \hat{\tau}_{t_{n+1}}^{k,p,K} \cdot 1_{\{G(S^{(k)}(t_n)) \geq \Psi_p(S^{(k)}(t_n) ; \hat{\theta}_{t_n}^{p,K} ) \}} \quad for \ n={0,\ldots,N-1} \\ 
\end{cases}
\end{split}
\end{equation*}
Where the estimator $\hat{\theta}_{t_n}^{p,K}$ is given by minimizing squared sample distance of the estimated continuation value and the realized continuation value:
\begin{align*}
\hat{\theta}_{t_n}^{p,K}= \argmin_{\theta \in \Theta_p} \sum_{k=1}^{K} 1_{\{G(S^{(k)}(t_n))>0\}} \bigg(G(S^{(k)}(\tau^{k,p,K}_{t_{n+1}}))  - \Psi_p(S^{(k)}(t_n) ; \theta ) \bigg)^2
\end{align*}
Finally, in analog with the LSM section the approximated time 0 price for the option is:
\begin{equation*}
U^{p,K}(0) = \max \{ G(S(0)), \frac{1}{K} \sum_{k=1}^{K} G(S^{(k)}(\hat{\tau}^{k,p,K}_{t_1}))]\}
\end{equation*}


\newpage


\section{Multilayer Perceptron Regression Pricing from Existing Methods}
The MLP pricing method from existing methods has a different approach than the other methods, because it is only data driven. The MLP could easily be used on real data, which is investigated in \parencite{GasparRaquel20}. We revisit the work from \parencite{HirsaAli2019}, where we try to extend the pricing method to options with two underlying risky assets. The model will be the classical Black-Scholes model presented in earlier chapters. By choosing this model the MLP pricing method is ready for investigation both for vanilla options and exotic options. We stress that the MLP pricing method is not restricted to the Black-Scholes model, but can be applied to other models such as Hesten, Variance Gamma, and real market data. The advantage of MLP is the fast pricing once trained. With the increased speed for pricing it can cost accuracy, especially if the data is sparse, which can arise when using the method for exotic options on real market data. For practical application the accuracy is severe if the predicted price leads to arbitrage. We consider the method for European call, American put, and American put mininum on two assets options presented in earlier chapters.\\

For deep learning the hyperparameters are important for finding the right model for pricing, where different choices will be presented empirically under training. For the given task polynomial regression can also be used, but we see later why MLP regression is preferred. The section is split into three sections \textsl{"Data"}, \textsl{"Training"}, and \textsl{"Performance"}.

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Data}
The generation of labels is the computationally expensive part of the MLP method, because the method needs enough samples to approximate the function $f^*$ well. The upside after generation of labels is that the method is computationally fast and easy to implement with basic knowledge of deep learning seen in chapter \ref{Chapter4}. The labels will be generated by existing methods presented in chapter \ref{Chapter2} and \ref{Chapter3}, where the input parameters will be sampled from a uniform distribution or quasi sampled with Halton sequences.\\

For both European and the American univariate contingent claims, the in-sample parameters will be the same, where we remember the 5 parameters for pricing the  univariate contingent claims\footnote{E.g. the European call proposition \ref{BS-price-EuroCall}}. Note for simulation of labels the European call and American put options are first order homogeneous function in $(S(0),K)$, hence, the valuation formulas can be modified:
$$\frac{c(S(0),K)}{K}=c(\frac{S(0)}{K},1) \quad and \quad \frac{P(S(0),K)}{K}=P(\frac{S(0)}{K},1)$$
The alternative representation above reduces the number of parameters needed for simulation to 4. Only moneyness ($\frac{S(0)}{K}$) is simulated instead of simulating both $S$ and $K$. This property is also shared for the American bivariate contingent claim. For the European call and American put the input matrix $\matr{X}$ is different combinations of the 4 parameters within the parameter range (table \ref{tab:vanillaParRange}), where it is assumed the number of trading days are 252 in a year. The parameter ranges are risk-free rate of return between 1-3 \%, maturity between 1 day to 3 years, moneyness between 0.8 and 1.2 and volatility between 0.05 and 0.5. \\

\begin{table}[th]
\caption[Parameter Ranges In-sample for MLP on Univariate Contingent Claims]{Parameter ranges for American put and European call options}
\label{tab:vanillaParRange}
\centering
\begin{tabular}{l l l l l}
\toprule
\textbf{Derivative} & \textbf{r} & \textbf{T} & \textbf{Moneyness} & $\sigma$ \\
\midrule
Euro. Call & 1\%-3\% & 1d - 3y & 0.8-1.2 & 0.05-0.5\\ 
Amer. Put & 1\%-3\% & 1d - 3y & 0.8-1.2 & 0.05-0.5\\ 
\bottomrule\\
\end{tabular}
\end{table}

The American put minimum option for two underlying assets will require additional parameters, because now we have two spots, two volatilities, and correlation between the assets. The first order homogeneity does also hold for the bivariate contingent claim.
$$\frac{P(S_1(0),S_2(0),K)}{K}=P(\frac{S_1(0)}{K}, \frac{S_2(0)}{K},1)$$
The parameters considered for the bivariate contingent claim are T, r, $Moneyness_1$, $Moneyness_2$, $\sigma_1$, $\sigma_2$ and $\rho$. So a total of 7 parameters and the given parameters ranges are given in table \ref{tab:ExoticParRange}.\\
   
\begin{table}[th]
\caption[Parameter Ranges In-sample for MLP on Bivariate Contingent Claim]{Parameter ranges for American put minimum on two assets}
\label{tab:ExoticParRange}
\centering
\begin{tabular}{l l l l l l l l l}
\toprule
\textbf{r} & \textbf{T} & $Moneyness_1$ & $Moneyness_2$ & $\sigma_1$ & $\sigma_2$ & $\rho$ \\
\midrule
1\%-3\% & 1d - 3y & 0.8-1.2 & 0.8-1.2 & 0.05-0.5 & 0.05-0.5 & (-0.5)-0.5\\
\bottomrule\\
\end{tabular}
\end{table}

The simulation of the parameter ranges for the training data set is done by quasi-random Halton sequences sampling to obtain lower discrepancy, where the test sets are sampled with random uniform sampling. The quasi random sampling is different from uniform random sampling, since the purpose is not to mimic randomness. Using Halton sequences sampling, the aim is to increase accuracy by generating points that are too evenly distributed to be random. The Halton sequence and uniform sampling generates points between 0 and 1, hence, we need to apply a transformation to have the parameter ranges:
$$Simulated \ point \cdot (range \ of \ parameter) + lower \ bound \ of \ parameter$$
After the simulation of the input parameters within the given ranges, the labels can be generated from existing methods. For the European call option the generation of labels is done by the classical B-S formula for call options given in proposition \ref{BS-price-EuroCall}. The B-S pricing formula is well-known and has an analytical solution, hence, it is relatively fast to generate labels in this model. The American options require numerical methods, therefore, the generation of labels is more computationally expensive. The labels for the American options are generated by CRR with 100 equidistant time-steps for the American put option with one underlying stock and by BEG for two underlying assets with 50 equidistant time-steps presented in chapter \ref{Chapter3}. When the data sets ($\matr{X}$, $\bm{y}$) are generated, we are left with a regression task. The regression task is to predict the price $\bm{y}$ from the input parameters $\matr{X}$.\\ 

The data sets generated for within the parameter ranges will be referred to as the in-sample data set. The training data set is an in-sample data set, which is split up into a training and validation data set in order to approximate model performance on the test sets. The training data set is used to update the parameters internally in the model, e.g. weights, biases, etc., and to fine-tune hyperparameters for choosing the optimal model design. To avoid overfitting and have good generalization models, the validation set is useful because the trained model has not seen the data before evaluation on the validation set. The validation set is randomly subsampled from the training set, where the validation data set constitutes 20 percent of the training set. To check the robustness of the regression, we choose to sample three test sets with one in-sample and two out-of-sample data sets. \\

The out-of-sample test data sets are simulated by uniform sampling adjusting the parameter range for either moneyness or maturity for the options (table \ref{tab:totalVanillaParRange}). The test data set has not been seen by the model in the training process, hence, we get an unbiased evaluation of the model. The aim with producing different test data sets is to measure the models' performance at interpolation and extrapolation. The data set for the bivariate American contingent claim will be similar to the American put by adjusting either the $moneyness_1$ and $moneyness_2$ or the maturity.\\

\begin{table}[th]
\caption{Parameter Ranges in Test Data Set for European Call and American Put Option}{Parameter ranges for European call and American put for test data sets}
\label{tab:totalVanillaParRange}
\centering
\begin{tabular}{l l l l l l l l }
\toprule
\textbf{Data set} & Derivative  & \textbf{r} & \textbf{T} & \textbf{Moneyness} & \textbf{$\sigma$} \\
\midrule
In-Sample & Euro. Call & 0.05-0.5 & 1d-3y & 0.8-1.2 & 1\%-3\%\\ 
Out-Of-Money & & 0.05-0.5 & 1d-3y & 0.6-0.8 & 1\%-3\%\\
Longer Maturity & & 0.05-0.5 & 3y-5y & 0.8-1.2 & 1\%-3\%\\
In-Sample & Amer. Put & 0.05-0.5 & 1d-3y & 0.8-1.2 & 1\%-3\%\\ 
In-The-Money & & 0.05-0.5 & 1d-3y & 0.6-0.8 & 1\%-3\%\\
Longer Maturity & & 0.05-0.5 & 3y-5y & 0.8-1.2 & 1\%-3\%\\
\bottomrule\\
\end{tabular}
\end{table}

\begin{figure}[th]
\centering
\includegraphics{Figures/marginalAmerPut.pdf}
\decoRule
\caption[Marginal Distributions for American Put]{Quasi random simulation with Halton sequences for input variables. The CRR model is used for generation of labels for the American put option.}
\label{fig:marginalAmerPut}
\end{figure}

In figure \ref{fig:marginalAmerPut} we have visualized a simulated training data set for the American put. The marginal distributions shown are for $300.000$ data samples ($\matr{X},y$) generated by Halton sequences and the CRR model. The marginal distributions for the features cover the parameter range almost uniformly and the simulated y lies with most values at zero and maximum at 0.387 rounded to three decimals. The marginal distributions show that we have successfully generated parameters in the given ranges and the parameters are evenly spaced in the ranges. In the model performance section, the out-of-sample and in-sample test data sets will be used to check the extrapolation and interpolation of the models. The test data sets are 60.000 generated data points with uniform sampling.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Training}\label{Training}
The aim of training is that the model will learn the pricing function $f^*$. Once the data set ($\matr{X}, \bm{y}$) is generated the model can be trained to approximate the true function $f^*$. We will present a standard linear model to compare the MLP with. We want to have a fast, robust, and accurate model after training on the training set. To train the model we need a measure for the error, where the mean squared error (MSE) for regression is applied. I.e. the cost function is chosen to be the empirical risk function with a quadratic loss function:
$$J(\theta)= \frac{1}{K} \sum_{k=1}^{K}(y_k-\hat{y}_k)^2$$
To avoid overfitting we regularize using early stopping with a patient of 5 epochs, but with a maximum of 100 epochs for computational reasons. To update the weights the Adam optimization algorithm is chosen.  The learning rate $\eta$ is found by hyperparameter tuning for the American put minimum on two assets option and chosen to be 0.001 for the univariate contingent claims.\\

\subsubsection{Hyperparameter Tuning}
A research area within deep learning is to fine-tune the hyperparameters to the specific task, where both manual and automated searches can be used. To choose the best hyperparameters there are several choices, where the most basic automated task is random search and grid search. The random search is to define a predefined range to pick randomly from. This method can be effective to discover new hyperparameter values or combination, but it can be considerably more computationally expensive than the grid search. The grid search is used for searching in a grid with each hyperparameter in each dimension. The grid search has fast computational time relative to the random search, but requires some expert knowledge about the hyperparameters.\\

We test empirically the best hyperparameters for the MLP, where the validation set is used. For hyperparameter tuning grid search is conducted to find the optimal set of hyperparameters, where we will look at data set size, learning rate, and batch size. \\

Firstly, the European call option regression is investigated, where we vary the data set size and batch size. The goal is to quantify how large a data set($d$) and batch size($b$) are needed for having a high quality model for the European option. The data sets for an European call option considered are in-sample data sets of size 1K, 10K, 100K, 300k and 1M data samples, where the validation set is subsampled from the training data set. The batch sizes are 64, 256 and 1024, i.e. we have the combinations expressed by the Cartesian product:
$$b \times d = \{(b, d) : b \in \{64, 256, 1024\} \ and \ d \in\{1K,10K,100K,300K,1M \} \}$$
By inspiration from \parencite{HirsaAli2019} we train a MLP model with 4 layers, 120 neurons in each hidden layer and 1 neuron in the output layer. In each layer we choose the activation function leaky ReLU with negative slope $\alpha$=0.01, which is one of the most popular choices for activation functions. The number of data samples is relevant for real data, because for real market data there is not unlimited market data available.\\

\begin{table}[th]
\caption{Grid Search for European Call}{Hyperparameter tuning of data set size and batch size for the European call option. The table shows validation loss in ascending order for different hyperparameter combinations and for the interested reader the Tensorboard is online (link \href{https://tensorboard.dev/experiment/8pxUoSDmTVGMOxpJWgiZsA/}{Tensorboard 1})}
\label{tab:hyperEuroC1}
\centering
\begin{tabular}{lllll}
\toprule
\textbf{Data set Size} & \textbf{Batch Size} & \textbf{Training Loss} & \textbf{Validation Loss} & \textbf{Time: min:sec}\\
\midrule
1M    & 256   & $8.094\cdot 10^{-7}$ & $8.7764\cdot 10^{-7}$ & 3:38 \\ 
300K  & 64    & $7.0562\cdot 10^{-7}$ & $1.0849\cdot 10^{-6}$ & 2:49 \\ 
1M    & 1024  & $1.1596\cdot 10^{-6}$ & $1.1578\cdot 10^{-6}$ & 5:58 \\ 
300K  & 256   & $9.579\cdot 10^{-7}$ & $1.3268\cdot 10^{-6}$ & 2:29 \\ 
300K  & 1024  & $1.5367\cdot 10^{-6}$ & $1.4138\cdot 10^{-6}$ & 9:28 \\ 
1M   & 64    & $3.4919\cdot 10^{-7}$ & $1.9197\cdot 10^{-6}$ & 8:24\\ 
100K  & 256   & $2.243\cdot 10^{-6}$ & $2.1192\cdot 10^{-6}$ & 1:02  \\ 
100K  & 64    & $1.9565\cdot 10^{-6}$ & $2.5738\cdot 10^{-6}$ & 1:01 \\ 
100K  & 1024  & $3.22\cdot 10^{-6}$ & $4.4754\cdot 10^{-6}$ & 2:00\\ 
10K  & 256   & $1.1179\cdot 10^{-5}$ & $1.0980\cdot 10^{-5}$ & 0:37 \\ 
10K   & 64    & $1.0043\cdot 10^{-5}$ & $1.9830\cdot 10^{-5}$ & 0:15 \\ 
1K   & 64    & $6.1389\cdot 10^{-5}$ & $7.8711\cdot 10^{-5}$ & 0:22\\ 
10K   & 1024 & $8.7067\cdot 10^{-5}$ & $8.1122\cdot 10^{-5}$ & 0:32  \\ 
1K   & 256   & $1.2032\cdot 10^{-4}$ & $1.2504\cdot 10^{-4}$ & 0:20    \\ 
1K    & 1024  & $7.5948\cdot 10^{-3}$ & $7.3595\cdot 10^{-3}$ & 0:08    \\ 
\bottomrule\\
\end{tabular}
\end{table}

Table \ref{tab:hyperEuroC1} shows that the the model performs well for in-sample data with 10K-300K data points. The biggest data set with 1M seems not to be worth the computational cost compared to the 300K data set. The model is only trained once on each data set, so there is some randomness on each run. The model to interpolate prices for European call options in-sample data does not significantly improve with gathering more data than 10K-300K data points, which is good news for using the method on real market data.  Beware that the simulated data can underestimate the validation error, because we have a controlled setup where the parameter range is within a predetermined range. For the controlled setup with simulated data we can choose arbitrary many data points, albeit making the method more computationally expensive. By weighting both the computational cost and accuracy, we choose to work with 300K data points and a batch size of 64 for the European option. \\

The European option needs fewer parameters compared to the American put on minimum of two assets, hence, the data set might need to be larger for that study. We conduct a large grid search with variations of the learning rate $\eta$, the batch size $b$ and the data set size $d$, where the grid of the Cartesian product of $\eta$, $b$ and $d$ is searched.
$$\eta \times b \times d = \{(\eta,b, d) : \eta \in \{0.0001, 0.001, 0.01 \}, \ b \in \{8, 64, 256, 512, 1024\} \ and \ d \in\{1K,100K,300K \} \}$$
Besides varying the hyperparameters for $\eta$, $b$ and $d$, the other hyperparameters are the same values as for the European call option.\\

\begin{table}[th]
\caption{Grid Search for American Put Minimum on two Stocks}{Hyperparameter tuning of data set size, learning rate and batch size for the American put bivariate contingent claim. The table shows the top 10 best performing combinations for the training loss and for the interested reader the Tensorboard is online (link \href{https://tensorboard.dev/experiment/ECWCP8nPTJWoXKVdBcQSaw/#scalars}{Tensorboard 2}) and the full table is given in appendix table \ref{tab:fullhyperAmerMin4}}
\label{tab:hyperAmerMin1}
\centering
\begin{tabular}{llllll}
\toprule
\textbf{Data set Size} & \textbf{Learning Rate} & \textbf{Batch Size} & \textbf{Train Loss} & \textbf{Val. Loss} & \textbf{Time: min:sec} \\
\midrule
300K     & 0.0001 & 64    & $1.350 \cdot 10^{-6}$   & $2.287 \cdot 10^{-6}$ & 4:50 \\ 
300K     & 0.001 & 64     & $2.170 \cdot 10^{-6}$   & $1.633 \cdot 10^{-6}$ & 2:13 \\ 
300K     & 0.0001 & 8     & $2.734 \cdot 10^{-6}$   & $2.638 \cdot 10^{-6}$ & 6:17\\ 
300K     & 0.0001 & 256   & $2.943 \cdot 10^{-6}$   & $2.850 \cdot 10^{-6}$ & 4:04\\ 
300K     & 0.001 & 256    & $3.685 \cdot 10^{-6}$   & $3.335 \cdot 10^{-6}$ & 1:26\\ 
100K     & 0.0001 & 64    & $4.081 \cdot 10^{-6}$   & $3.575 \cdot 10^{-6}$ & 1:44\\ 
100K     & 0.001 & 64     & $4.817 \cdot 10^{-6}$   & $5.615 \cdot 10^{-6}$ & 0:57\\ 
100K     & 0.0001 & 256   & $5.182 \cdot 10^{-6}$   & $6.367 \cdot 10^{-6}$ & 1:52\\ 
300K     & 0.0001 & 1024  & $5.534 \cdot 10^{-6}$   & $6.473 \cdot 10^{-6}$ & 3:42\\ 
300K     & 0.001 & 8      & $5.798 \cdot 10^{-6}$   & $4.284 \cdot 10^{-6}$ & 5:40\\ 
\bottomrule\\
\end{tabular}
\end{table}

Looking at table \ref{tab:hyperAmerMin1} the best configuration for the training loss is for $(\eta=0.0001, b=64, d=300K)$, where the best configuration in terms of validation loss is $(\eta=0.001, b=64, d=300K)$. The computational burden increases when training with a bigger data set, smaller learning rate, and smaller batch sizes. The learning rate affects the computational speed because we are training with early stopping regularization. The batch size affects also the computational speed, since a high batch size gives fewer iterations per epoch. Overall, the data set with 300K performed best, which is expected, since a bigger data set gives more learning opportunities. Considering computational cost and the loss; the 300K data sets, the learning rate $\eta=0.001$ and a batch size of 64 are preferred.\\

Hyperparameter tuning is highly computationally expensive and there are many combinations to test out. With grid search we have seen a possible optimal choice for the learning rate, batch size, and data set size for the European call option and the American put minimum on two assets option. The American put option uses the same parameters as the European option, therefore,  we choose the same hyperparameters for the univariate contingent claims. Below, we will try a different model than MLP to see if we really need deep learning for this method. 

\subsubsection{Polynomial Regression}
To compare MLP and Polynomial regression the data set and the performance metrics are the same, but the model training is obviously different. The chosen data set size is 300.000 samples and the performance metric is MSE. For simplicity the European call option is investigated, where we fit polynomials up to the sextic polynomial for comparison of the model capacity and fit.  
$$y_i=\beta_0 + \beta_1 \cdot x_i + \cdots + \beta_n \cdot x_i^n + \epsilon_i \quad where \ n=1,2,\ldots,6$$
Plotting the fit actual vs predicted target variable (figure \ref{fig:PolynomialEuroC}), it is clear that the in-sample fit improves with the increased model capacity. The linear regression is too simple for pricing European option, but the sextic polynomial actually performs better than the MLP for the in-sample validation set (table \ref{tab:euroPerformance}). Keep in mind that we do not only want good interpolation, but we also want good extrapolation for our fitted model. The out-of-sample data will reveal if the high order polynomial or MLP have overfitted the data.\\

\begin{figure}[H]
\centering
\includegraphics{Figures/polynomialEuroC.png}
\decoRule
\caption[Polynomial Regression Predictions Vs. Actual Prices]{Predicted price based on polynomial regression of varying degree}
\label{fig:PolynomialEuroC}
\end{figure}

\begin{table}[th]
\caption{European Call Validation Error}{Validation error of polynomial regression and MLP for the European call option}
\label{tab:euroPerformance}
\centering
\begin{tabular}{l l}
\toprule
\textbf{Model} & \textbf{Validation Loss} \\
\midrule
Linear  poly. & 0.000631 \\
Quadratic poly.  & 0.000069 \\
Cubic poly. & 0.000013\\
Quartic poly.  & 0.000004 \\
Quintic poly.  & 0.000002 \\
Sextic poly. & 0.000001\\
MLP        & 0.000003\\
\bottomrule\\
\end{tabular}
\end{table}
Looking closer at table \ref{tab:euroPerformance} to compare the performance of each model. The table confirms that the linear regression has a worse fit than the other models with higher capacity. The difference on the MLP and best performing polynomial model are less than $2\cdot 10^{-6}$. The difference is almost negligible, so the fit for MLP and polynomial regression of degree 4-6 all perform well on the in-sample training data in terms of the validation loss.

%-----------------------------------
%	SUBSECTION 3
%-----------------------------------
\subsection{Performance}
The model performance is evaluated by MSE, RMSE, MAE and coefficient of determination, where all the measures evaluate how close the model predictions are with the actual targets. The first three measure ranges are $\mathbb{R}^+$, where the goal is to have the lowest value possible. MSE close to 0 means that the model predictions do not differ a lot from the observed targets. The RMSE and MAE are the same type of measure, but the deviation is measured slightly different. The coefficient of determination has range $(-\infty, 1]$, where a higher value indicates a better model. Coefficient of determination provides a measure of how well observed targets are predicted by the model based on the proportion of total variation of targets explained by the model.

\subsubsection{European Call Option}
The European call option is trained with the algorithm and hyperparameters described in the training section (section \ref{Training}). By the hyperparameter investigation we choose a batch size of 64 and a data set of 300K samples. We compare the MLP regression with the polynomial regression. Table \ref{tab:ComparePolyWithMLP} shows that the MLP is superior at extrapolating, because the MLP performs better on all metrics on the out-of-sample test data sets compared to fitted polynomial regressions with a degree between 1 and 6\footnote{The out-of-sample fits for the European call with polynomial regression are illustrated in figure \ref{fig:MLPsEuroCOutOfMoney} and figure \ref{fig:MLPsEuroCLongMaturity}}. For the in-sample test data set the sextic polynomial regression and MLP perform almost equally well. Note, the in-sample test data is similar to the in-sample training data, hence, we expect the same magnitude of error for the test set as for the training set for in-sample testing. The performance measures show that the polynomial regression, that was performing well on the in-sample data set, was due to overfitting, because the high order polynomial regression performs poorly on out-of-sample data (table \ref{tab:ComparePolyWithMLP}). For the sextic polynomial regression, we see a negative coefficient of determination, which means the model performs worse than the model that predicts the sample mean. \\

\begin{table}[H]
\caption{Test Error for European Call Option}{Performance comparison of MLP and polynomial regression for the European call option. Shown the best performing regressions in the linear model and the worst performing in terms of MSE for in-sample and out-of-sample test data sets (The full table for polynomial regression is found in appendix table \ref{tab:fullEuroCall})}
\label{tab:ComparePolyWithMLP}
\centering
\begin{tabular}{l l l l l l l }
\toprule
\textbf{Model} & \textbf{Data set} & \textbf{MSE} & \textbf{RMSE} & \textbf{MAE} & \textbf{Coefficient of Determination} \\
\midrule
MLP & In-Sample & 0.000000 & 0.000629 & 0.000486 & 0.999961\\
& Out-of-Money & 0.000007 & 0.002644 & 0.001551 & 0.995911\\
& Long Maturity & 0.000197 & 0.014048 & 0.010061 & 0.986518\\
Sextic poly. & In-Sample & 0.000001 & 0.000958 & 0.000591 & 0.999909\\
Linear poly. &  & 0.000636 & 0.025212 & 0.018326 & 0.936628\\
Quadratic poly. & Long Maturity & 0.001196 & 0.034577 & 0.026287 & 0.918316\\
Sextic poly. &  & 0.043361 & 0.208233 & 0.111190 & -1.962442\\
Quadratic poly. & Out-Of-Money & 0.000767 & 0.027694 & 0.022203 & 0.551246\\
Linear poly. &  & 0.005772 & 0.075973 & 0.060936 & -2.377251\\
\bottomrule\\
\end{tabular}
\end{table}

The MLP has high predictive strength compared to the polynomials, because it performs well, also on out-of-sample data sets. The MLP shows that it predicts out-of-money call options with less than a MSE value of $10^{-5}$, where the fit for the long maturity test set has a higher MSE. The European options are liquid in the markets, hence, the pricing method can easily be trained on real data. The MLP fit for in-sample data on the European call option is visualized in figure \ref{fig:MLPInSampleEuroC}, which shows $\frac{c(S_0,K)}{K}$ predicted from the model and observed target values $\bm{y}$. The figure shows an overall close fit. For the remaining part of this section, only the MLP regression will be considered, because the polynomial regression has shown bad performance for out-of-sample data.

\begin{figure}[th]
\centering
\includegraphics{Figures/PredictionEuroC.pdf}
\decoRule
\caption[MLP Performance for In-sample Data set European Call]{MLP performance for in-sample data set European call}
\label{fig:MLPInSampleEuroC}
\end{figure}


\subsubsection{American Put Option}
The American put option is priced with the same MLP algorithm as for the European call. Table \ref{tab:AmerPerformanceComparision} shows once again a good fit, hence, we believe we have a high quality model for the American put option\footnote{The out-of-sample fits for the American put with MLP regression are illustrated in figure \ref{fig:MLPsAmerPOutMoney} and figure \ref{fig:MLPsAmerPLongT}}.

\begin{table}[th]
\caption{MLP Performance on American Put Option}
\label{tab:AmerPerformanceComparision}
\centering
\begin{tabular}{l l l l l l l }
\toprule
\textbf{Data set} & \textbf{MSE} & \textbf{RMSE} & \textbf{MAE} & \textbf{$R^2$} \\
\midrule
In-Sample & 0.000002 & 0.001562 & 0.001278 & 0.999634\\
In-The-Money & 0.000012 & 0.003519 & 0.002290 & 0.995778\\
Longer Maturity & 0.000193 & 0.013894 & 0.009213 & 0.980835\\
\bottomrule\\
\end{tabular}
\end{table}

\subsubsection{American Put on Minimum of two Assets Option}
The American put on minimum of two assets option has almost double the number of parameters compared to the univariate contingent claims. Hence, the performance might produce a slightly higher MSE. Table \ref{tab:AmerMinPerformanceComparision} and figure \ref{fig:MLPInSampleAmerMin} show that the MLP also fits the American put minimum option on two assets well. The performance seems similar to the univariate case, hence we see that the MLP can easily fit bivariate contingent claims as well. \\ 

\begin{table}[th]
\caption{Performance of the Bivariate American Put Contingent Claim}
\label{tab:AmerMinPerformanceComparision}
\centering
\begin{tabular}{l l l l l l l }
\toprule
\textbf{Data set} & \textbf{MSE} & \textbf{RMSE} & \textbf{MAE} & \textbf{$R^2$} \\
\midrule
In-Sample & 0.000002 & 0.001361 & 0.001008 & 0.999794\\
In-The-Money & 0.000143 & 0.011973 & 0.009208 & 0.964370\\
Longer Maturity & 0.000112 & 0.010565 & 0.007319 & 0.989863\\
\bottomrule\\
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics{Figures/inSampleAmerMinP.pdf}
\decoRule
\caption[MLP Performance for In-sample Data Set American Put on minimum on two Assets]{MLP performance for in-sample data set American bivariate contingent claim}
\label{fig:MLPInSampleAmerMin}
\end{figure}

The MLP has shown good performance in terms of our performance measures on all the derivatives considered. Next chapter will compare this MLP model (henceforth referred to as MLP II) with the closed form solutions, binomial model, LSM and LSM MLP (henceforth referred to as MLP I). All the models considered in this section had good performances on the in-sample data set except for the too simple models; linear regression and quadratic polynomial regression. The MLP II was superior for out-of-sample data sets compared to the polynomial regression in terms of our performance measures.\\

The MLP II is not based on a model, it is only trained on the available data. This feature makes the MLP II pricing method versatile, because the MLP could also be used for actual market data or different models to learn patterns. The fact that the method can be extended to real market data is already shown in \parencite{GasparRaquel20}. This section showed how the MLP can be trained to pricing derivatives with Black-Scholes theory and that the MLP is preferred over polynomial regression.









