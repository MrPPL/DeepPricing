% Appendix Template

\chapter{Additional Material for CRR, LSM and MLPs I} % Main appendix title

\label{AppendixC} % Change X to a consecutive letter; for referencing this appendix elsewhere, use \ref{AppendixX}

\section{Moment Matching CRR}\label{CRRMM}
In the CRR the stock multiplication factor for up and down movement is chosen to match the two first moments of the lognormal distribution. By matching the first moments the underlying discrete binomial stochastic process for the stock converge toward the continuous time lognormal distribution for sufficient large equidistant time-steps N. Hence the CRR model will coincide with the Black-Scholes model.\\

The SDE for the Black Scholes under the equivalent martingale measure Q:
$$dS(t)=rS(t)dt + \sigma S(t) dW(t)$$
Using It√¥'s lemma:
\begin{equation}\label{lnGBM}
d\ln(S(t)))=(r-\frac{1}{2}))dt + \sigma dW_t
\end{equation}
The solution of equation \ref{lnGBM} is then:
$$S(t)=S(0)\exp\bigg((r-\dfrac{1}{2}\sigma)t+ \sigma W(t) \bigg)$$
Note that $W(t)\sim \mathcal{N}(0,t)$ implies:
$$\ln(\dfrac{S(t)}{S(0)}) \sim \mathcal{N}((r-\dfrac{1}{2}\sigma^2)t, \sigma^2 t)$$
The two first moments for the lognormal distribution is then:
\begin{equation}\label{lnMoments}
\begin{split}
E[\dfrac{S(t)}{S(0)}]=\exp(rt)\\
E[(\dfrac{S(t)}{S(0)})^2]=\exp(t\cdot (2r + \sigma^2))
\end{split}
\end{equation}
The above derivation can be used for any time interval.\\

The binomial lattice model has two discrete outcomes from each state, hence the moments are:
\begin{equation}\label{binMoments}
\begin{split}
E[\dfrac{S(t_{n+1})}{S(t_{n})}]=u \cdot Q(\dfrac{S(t_{n+1})}{S(t_{n})} = u) + d \cdot Q(\dfrac{S(t_{n+1})}{S(t_{n})} = d) = u \cdot q + d \cdot (1-q)\\
E[(\dfrac{S(t_{n+1})}{S(t_{n})})^2]=u^2 \cdot q + d^2 \cdot (1-q)
\end{split}
\end{equation}
We match the moments (equations \eqref{lnMoments} and \eqref{binMoments}) and get two equations with two unknowns, where the time-step is chosen to be $\Delta t$
\begin{align*}
\exp(r \Delta t)=u \cdot q + d \cdot (1-q) \quad (i)\\
\exp(\Delta t \cdot (2r + \sigma^2))=u^2 \cdot q + d^2 \cdot (1-q) \quad (ii)
\end{align*}
Multipling $(i)$ with u+d and recognizing $(ii)$:
\begin{align*}
(u+d)\exp(r \Delta t)&=u^2 \cdot q + d^2 \cdot (1-q) + ud\\
\Rightarrow (u+d)\exp(r \Delta t)&\overset{(ii)}{=}\exp(\Delta t \cdot (2r + \sigma^2)) + u d
\end{align*}
Remember we choose $u= \frac{1}{d}$ hence we arrive at a quadratic equation by some algebra.
\begin{align*}
u^2 - u\bigg(\exp(-r \Delta t) + \exp(\Delta t(r+\sigma^2))\bigg)+1=0
\end{align*}
We are interested in that the binomial model converge toward the Black-Scholes model, hence we are looking at small time increment $\Delta t$. This justify the Taylor approximation of the exponential function around zero.
\begin{align*}
\exp(r \Delta t + \sigma^2 \Delta t) \approx 1 + (r+\sigma^2)\Delta t + O(t^2)
\end{align*}
By Taylor approximation we arrive at a simpler quadratic equation:
\begin{equation*}
u^2-u(2+\sigma^2 \Delta t) + 1 = 0
\end{equation*}
Solving the quadratic equation above gives
\begin{align*}
u&=1+\dfrac{1}{2} \sigma^2 \Delta t \pm  \dfrac{\sqrt{(2+\sigma^2 \Delta t)^2 - 4}}{2}\\
&=1+\dfrac{1}{2} \sigma^2 \Delta t \pm  \dfrac{\sqrt{(4+\sigma^4 \Delta t^2 + 4 \sigma^2 \Delta t - 4}}{2}\\
&=1+\dfrac{1}{2} \sigma^2 \Delta t \pm \frac{1}{2} \sigma \sqrt{\Delta t} \sqrt{\sigma^2 \Delta t + 4}\\
&\approx 1+\dfrac{1}{2} \sigma^2 \Delta t \pm \sigma \sqrt{\Delta t}\\
&\approx \exp(\pm \sigma \sqrt{\Delta t})
\end{align*}
Both approximations use that the time-step is small.
\section{Convergence for LSM and MLPs I}\label{Convergence}
\subsection{LSM}
In the rigorous approach in \parencite{analysisLSM} they show convergence results for the optimal value process or the Snell envelope $U$. We will present that $U(0)^{m,K}$ converges almost surely to $U(0)^{m}$ for K goes to infinity, i.e. the approximate value process by simulation and regression on a finite set of functions converge to the approximated value process with truncated orthogonal basis by letting the sample size go to infinity. Furthermore it can be shown that $U(0)^{m}$ converge to $U(0)$ for m goes to infinity. The second result is that the regressed value function converge to the expected continuation value by letting the number of basis function goes to infinity. The latter result is shown using the expected continuation values.
\begin{theorem}\label{LSMConvergence1}
Assume the sequence $(e_{j}(S(t_n)))_{j\geq 1}$ is total in $L^2(\sigma(S(t_n)))$ for $n=1,\ldots,N-1$. Then for $n=0,\ldots,N$ we have
$$\lim_{m\to +\infty} E^Q[G(S(\tau_{t_n}^{[m]})) |\mathcal{F}_{t_n}]=E^Q[G(S(\tau_{t_n})) |\mathcal{F}_{t_n}]$$
in $L^2$
\begin{proof}
The proof is given by induction, where the orthogonal basis is total in $L^2$ is important, because $||P^m_{t_n}(E^Q[G(S(\tau_{t_{n+1}}))|\mathcal{F}_{t_n}])- E^Q[G(S(\tau_{t_{n+1}}))|\mathcal{F}_{t_n}]||_2 \to 0 \quad for \ m \to \infty$.
(more details on p. 6-7 \parencite{analysisLSM})
\end{proof}
\end{theorem}

The former result is also shown in \parencite{analysisLSM}.
\begin{theorem}\label{LSMConvergence2}
Assume the sequence $(e_{j}(S(t_n)))_{j\geq 1}$ is total in $L^2(\sigma(S(t_n)))$ for $n=1,\ldots,N-1$ and if $\sum_{j=1}^{m} \lambda_j e_{j}(S(t_n))=0 \ a.s.$ then $\lambda_j=0$ for $n=1,\ldots,N-1$, $m\geq 1$ and $j=1,\ldots,m$. Furthermore assume that $Q(\alpha_{t_n} \cdot e(S(t_n))=G(S(t_n)))=0$.\\
Then $U^{m,K}(0)$ converges almost surely to $U^{m}(0)$ as K goes to infinity.\\
The proof is out of scope for this thesis, see the article \parencite{analysisLSM} for a proof in details. 
\end{theorem}

The two convergence results show that the convergence for the LSM algorithm, hence the LSM will approximate the optimal value process well for sufficient large sample sets and enough basis functions.

\subsection{MLPs I}
The MLPs regression method enjoys the same convergence results presented for the LSM algorithm, i.e.
\begin{theorem}\label{NNConvergence1}
Assume that 
$$E[\max_{0\leq t_n \leq T} |G(S(t_n))|^2]< \infty$$. 
Then $\lim_{p \to \infty} E^Q[G(S(\tau^{p}_{t_n}))| \mathcal{F}_{t_n}]= E[G(\tau_{t_n})|\mathcal{F}_{t_n}]$ in $L^2(\Omega)$ for all $n \in \{1,2,\ldots, N\}$\\
Proof p. 7-8 \parencite{Lelong19}
\end{theorem}
The above theorem states convergence of the neural network approximation, i.e. $U^{p}(0) \to U(0)$ which is similar to the convergence result for LSM. 
\begin{theorem}{\textbf{Strong law of large numbers: }}\label{NNConvergence2}
Assume
\begin{enumerate}
\item[A1:] For every $p\in \mathbb{N}$, $p>1$, there exist $q \geq 1$ and $\kappa_p>0$ such that
$$\forall s \in \mathbb{R}^{R}, \forall \theta \in \Theta_p, \quad |\Psi_p(s,\theta)| \leq \kappa_p (1+|s|^q) $$
Moreover $\forall n \in \{1,2,\ldots, N-1\}$, a.s. the random functions $\theta \in \Theta_p \mapsto \Psi_p(S(t_n), \theta)$ are continuous. Note $\Theta_p$ is a compact set, hence the continuity is uniform.
\item[A2:] For $q$ defined in A1, $E^Q[|S(t_n)|^{2q}]<\infty \quad \forall n \in \mathbb{N} \cup 0$
\item[A3:] $\forall p \in \mathbb{N}$, $p>1$ and $\forall n \in \{1,2,\ldots, N-1\}$, 
$$P(S(t_n)=\Psi_{p}(S(t_n);\theta_{t_n}^{p})=0$$
\item[A4:] $\forall p \in \mathbb{N}$, $p>1$ and $\forall n \in \{1,2,\ldots, N-1\}$, if $\theta^{1}$ and $\theta^{2}$ solves 
\begin{align*}
\argmin_{\theta \in \Theta_p} E^Q[|\Psi_p(S(t_n); \theta)- S(\tau_{t_{n+1}}^{p})|^2]
\end{align*}
then $\Psi_p(s,\theta^{1})=\Psi_p(s,\theta^{2})$ for almost all x.
\end{enumerate}
If A1-A4 hold, then for $\zeta\in \{1,2\}$ and every $n\in \{1,2,\ldots,N\}$
\begin{equation}
\lim_{K\to \infty} \dfrac{1}{K} \sum_{k=1}^{K} \bigg(G(S(\hat{\tau}_{t_n}^{k,p,K}))\bigg)^{\zeta} = E[\bigg(G(S(\tau_{t_n}^{p}))\bigg)^{\zeta}] \quad a.s.
\end{equation}
Proof p. 13-14 \parencite{Lelong19}
\end{theorem}
The last result is the strong law of large numbers for the value function, i.e. $U^{p,K}(0) \to U^{p}(0) \ a.s. \quad for \ K \to \infty$

\section{LSM Lower Bound}\label{LSMLowerBound}
The LSM approach gives a lower bound for the true price of the option given optimal stopping choice:
\theoremstyle{proposition}
\begin{proposition}{}\label{Lower-Bound-LSM}
\textbf{Lower Bound To True Value:} For any finite choice of M, K, and vector $\theta\in \mathbb{R}^{M \times (K-1)}$ representing the coefficients for the M basis functions at each of the K-1 early exercise dates, let $LSM(\omega;M,K)$ denote the discounted cash flow resulting from the following the LSM rule of exercising when the immediate exercise value is positive and greater than or equal to $\hat{F}_{M}(\omega_{l};t_{k})$ as defined by $\theta$. Then the following inequality holds almost surely,
$$V(X)\geq \lim_{N\to \infty} \dfrac{1}{N}\sum_{i=1}^{N} LSM(\omega_i;M,K)$$
\null \hfill (p. 124 \parencite{LSM})
\end{proposition}